import streamlit as st
import time
import os
from scraper import fetch_page, get_all_website_links, is_valid_url, process_content, hash_content
from document_processor import create_document
from urllib.parse import urlparse
from bs4 import BeautifulSoup

# --- Streamlit Config ---
st.set_page_config(
    page_title="Web KazÄ±ma AracÄ±",
    page_icon="ğŸ•¸ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS stillerini ekleyelim
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1.5rem;
    }
    .sub-header {
        font-size: 1.8rem;
        color: #0D47A1;
        margin-top: 1.5rem;
        margin-bottom: 1rem;
    }
    .success-message {
        padding: 10px;
        background-color: #d4edda;
        color: #155724;
        border-radius: 5px;
        margin-bottom: 10px;
    }
    .warning-message {
        padding: 10px;
        background-color: #fff3cd;
        color: #856404;
        border-radius: 5px;
        margin-bottom: 10px;
    }
    .error-message {
        padding: 10px;
        background-color: #f8d7da;
        color: #721c24;
        border-radius: 5px;
        margin-bottom: 10px;
    }
    .info-card {
        border-left: 5px solid #1E88E5;
        padding: 15px;
        background-color: #f8f9fa;
        margin: 10px 0;
    }
    .settings-section {
        background-color: #f8f9fa;
        padding: 20px;
        border-radius: 10px;
        margin-bottom: 20px;
    }
    .results-section {
        background-color: #f8f9fa;
        padding: 20px;
        border-radius: 10px;
        margin-top: 20px;
    }
</style>
""", unsafe_allow_html=True)

seen_hashes = set()

# --- Sidebar ---
with st.sidebar:
    st.image('https://www.svgrepo.com/show/374122/spider.svg', width=100)
    st.markdown("<h1 style='text-align: center;'>Ayarlar</h1>", unsafe_allow_html=True)
    
    with st.expander("ğŸ“Œ Genel Ayarlar", expanded=True):
        mode = st.radio("KazÄ±ma Modu", ["Tek URL", "TÃ¼m Site"], 
                         help="Tek bir sayfayÄ± ya da tÃ¼m site sayfalarÄ±nÄ± kazÄ±mak iÃ§in seÃ§in")
        
        url = st.text_input("URL girin:", 
                            placeholder="https://example.com",
                            help="KazÄ±ma iÅŸleminin baÅŸlayacaÄŸÄ± URL")
        
        save_dir = st.text_input("KayÄ±t KlasÃ¶rÃ¼:", 
                                 value="kazima_sonuclari",
                                 help="DOCX dosyalarÄ±nÄ±n kaydedileceÄŸi klasÃ¶r")
    
    with st.expander("ğŸ” Element SeÃ§enekleri", expanded=True):
        col1, col2 = st.columns(2)
        opts = {}
        
        with col1:
            for lvl in range(1, 4):
                opts[f'h{lvl}'] = st.checkbox(f'H{lvl} BaÅŸlÄ±klar', value=True, key=f'h{lvl}_header')
            opts['p'] = st.checkbox('Paragraflar', value=True, key='paragraph_option')
            opts['div'] = st.checkbox('Div Ä°Ã§erikleri', value=False, key='div_content_option')
            if opts['div']:
                opts['filter_divs'] = st.checkbox('Gereksiz Divleri Filtrele', 
                                                value=True,
                                                key='filter_divs_option',
                                                help="MenÃ¼, sidebar, popup gibi gereksiz iÃ§erikleri filtreler")
        
        with col2:
            for lvl in range(4, 7):
                opts[f'h{lvl}'] = st.checkbox(f'H{lvl} BaÅŸlÄ±klar', value=False, key=f'h{lvl}_header2')
            opts['lists'] = st.checkbox('Listeler', value=True, key='lists_option')
            opts['headers'] = st.checkbox('Header', value=False, key='header_option')
            opts['footers'] = st.checkbox('Footer', value=False, key='footer_option')
            opts['span'] = st.checkbox('Span Ä°Ã§erikleri', value=False,
                                     key='span_content_option',
                                     help="Sadece Ã¶zel iÃ§erikli span'larÄ± alÄ±r")
        
        st.divider()
        
    
    if mode == "TÃ¼m Site":
        with st.expander("ğŸŒ Site KazÄ±ma AyarlarÄ±", expanded=True):
            depth = st.slider("Link DerinliÄŸi", 1, 5, 2, 
                             help="KaÃ§ seviye derinliÄŸe kadar linkleri takip edeceÄŸinizi belirler")
            maxp = st.slider("Maksimum Sayfa", 10, 500, 50, 
                            help="KazÄ±nacak maksimum sayfa sayÄ±sÄ±")
    
    st.divider()
    start_button = st.button("ğŸš€ KazÄ±mayÄ± BaÅŸlat", use_container_width=True)
    
    st.markdown("""
    <div style="
        background-color: #333;
        color: white;
        padding: 20px;
        border-radius: 10px;
        margin-bottom: 20px;
    ">
    <h3>Ä°puÃ§larÄ±</h3>
    <ul>
        <li>Site kazÄ±mada derinlik artÄ±ÅŸÄ± iÅŸlem sÃ¼resini uzatÄ±r</li>
        <li>BazÄ± siteler otomatik kazÄ±mayÄ± engeller endiÅŸelenmeyin</li>
        <li>Benzer iÃ§erikler otomatik atlanÄ±r</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)

# --- Main Content ---
st.markdown("<h1 class='main-header'>ğŸ•¸ï¸ Web KazÄ±ma AracÄ±</h1>", unsafe_allow_html=True)

tab1, tab2, tab3 = st.tabs(["ğŸ“‹ Ä°ÅŸlem", "ğŸ“Š SonuÃ§lar", "â“ YardÄ±m"])

with tab1:
    if not start_button:
        st.info("ğŸ‘ˆ AyarlarÄ± yapÄ±landÄ±rÄ±n ve baÅŸlatmak iÃ§in soldaki butona tÄ±klayÄ±n.")
        
        col1, col2, col3 = st.columns([1,2,1])
        with col2:
            st.image("https://www.svgrepo.com/show/374116/spider-web.svg", width=300)
    
    if start_button:
        if not is_valid_url(url):
            st.error("âŒ GeÃ§erli bir URL girmelisiniz. Ã–rnek: https://example.com")
        else:
            tasks = []
            visited = set()
            domain = urlparse(url).netloc
            
            with st.spinner("ğŸ” URL'ler toplanÄ±yor..."):
                if mode == "Tek URL":
                    html = fetch_page(url)
                    if html:
                        tasks.append((url, html))
                else:
                    status_text = st.empty()
                    progress_bar = st.progress(0)
                    to_visit = [(url, 0)]
                    
                    while to_visit and len(visited) < maxp:
                        current_url, level = to_visit.pop(0)
                        if current_url in visited or level > depth:
                            continue
                            
                        visited.add(current_url)
                        status_text.text(f"ğŸ” Ä°nceleniyor: {current_url}")
                        
                        html = fetch_page(current_url)
                        if html:
                            tasks.append((current_url, html))
                            
                            if level < depth:
                                new_links = get_all_website_links(current_url, domain)
                                for link in new_links:
                                    if link not in visited and is_valid_url(link):
                                        to_visit.append((link, level + 1))
                        
                        progress_bar.progress(min(1.0, len(visited)/maxp))
                        status_text.text(f"ğŸ“„ Toplam {len(tasks)} sayfa bulundu ({len(visited)} URL ziyaret edildi)")
                        time.sleep(0.1)
            
            if tasks:
                st.markdown(f"### ğŸ“ƒ Toplam {len(tasks)} sayfa iÅŸlenecek")
                
                succ = 0
                fail = 0
                results = []
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                result_area = st.container()
                
                for i, (u, html) in enumerate(tasks):
                    status_text.text(f"âš™ï¸ Ä°ÅŸleniyor: {u}")
                    soup = BeautifulSoup(html, 'html.parser')
                    cont = process_content(soup, opts)
                    hash_val = hash_content(cont)
                    
                    if hash_val in seen_hashes:
                        results.append(("warning", f"âš ï¸ Benzer iÃ§erik atlandÄ±: {u}"))
                        fail += 1
                    elif len(cont) > 1:
                        seen_hashes.add(hash_val)
                        fname = create_document(cont, save_dir, u, i)
                        results.append(("success", f"âœ… Kaydedildi: {u} â†’ {fname}"))
                        succ += 1
                    else:
                        results.append(("error", f"âŒ Ä°Ã§erik bulunamadÄ±: {u}"))
                        fail += 1
                    
                    progress_bar.progress((i+1)/len(tasks))
                    
                    with result_area:
                        for res_type, res_text in results[-5:]:
                            if res_type == "success":
                                st.success(res_text)
                            elif res_type == "warning":
                                st.warning(res_text)
                            else:
                                st.error(res_text)
                
                status_text.text("")
                st.balloons()
                
                st.markdown("## ğŸ“Š Ä°ÅŸlem Ã–zeti")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Toplam Ä°ÅŸlenen", len(tasks))
                with col2:
                    st.metric("BaÅŸarÄ±lÄ±", succ)
                with col3:
                    st.metric("BaÅŸarÄ±sÄ±z", fail)
                
                st.success(f"âœ… Ä°ÅŸlem tamamlandÄ±! Dosyalar '{save_dir}' klasÃ¶rÃ¼ne kaydedildi.")
            else:
                st.error("âŒ Ä°ÅŸlenecek sayfa bulunamadÄ±. LÃ¼tfen farklÄ± bir URL deneyin.")

with tab2:
    if os.path.exists(save_dir):
        files = [f for f in os.listdir(save_dir) if f.endswith('.docx')]
        if files:
            st.markdown(f"## ğŸ“ KayÄ±tlÄ± Dosyalar ({len(files)})")
            
            file_data = []
            for i, file in enumerate(sorted(files)):
                file_size = os.path.getsize(os.path.join(save_dir, file)) / 1024
                created_time = os.path.getctime(os.path.join(save_dir, file))
                file_data.append({
                    "No": i+1,
                    "Dosya AdÄ±": file,
                    "Boyut": f"{file_size:.1f} KB",
                    "OluÅŸturulma": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(created_time))
                })
            
            st.dataframe(file_data, use_container_width=True)
            
            st.markdown("### ğŸ“ˆ Dosya Ä°statistikleri")
            total_size = sum([os.path.getsize(os.path.join(save_dir, f)) for f in files]) / 1024
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Toplam Dosya", len(files))
            with col2:
                st.metric("Toplam Boyut", f"{total_size:.1f} KB")
        else:
            st.info("ğŸ“‚ HenÃ¼z kaydedilmiÅŸ dosya bulunmuyor.")
    else:
        st.info("ğŸ“‚ KayÄ±t klasÃ¶rÃ¼ henÃ¼z oluÅŸturulmadÄ±.")

with tab3:
    st.markdown("## ğŸ” Web KazÄ±ma AracÄ± KullanÄ±m KÄ±lavuzu")
    
    st.markdown("""
    ### ğŸ“Œ Genel KullanÄ±m
    1. **URL GiriÅŸi**: KazÄ±ma yapÄ±lacak web sitesinin URL'sini girin
    2. **KazÄ±ma Modu**: Ä°ki seÃ§enek bulunur:
       - **Tek URL**: Sadece girilen URL'yi kazÄ±r
       - **TÃ¼m Site**: Girilen URL'den baÅŸlayarak siteyi dolaÅŸÄ±r ve bulduÄŸu sayfalarÄ± kazÄ±r
    
    ### ğŸ”§ Ã–zelleÅŸtirme SeÃ§enekleri
    - **Element SeÃ§imi**: Hangi HTML elementlerinin kazÄ±nacaÄŸÄ±nÄ± seÃ§in
      - **H1-H6**: FarklÄ± seviyedeki baÅŸlÄ±klar
      - **Paragraflar**: `<p>` elementleri
      - **Div Ä°Ã§erikleri**: `<div>` elementleri
      - **Span Ä°Ã§erikleri**: Ã–zel iÃ§erikli `<span>` elementleri (soru-cevap bÃ¶lÃ¼mleri gibi)
      - **Listeler**: `<ul>` ve `<ol>` liste elementleri
      - **Header/Footer**: Sayfa Ã¼stÃ¼ ve altÄ± bÃ¶lÃ¼mler
    
    ### ğŸ“‹ SonuÃ§lar
    - KazÄ±nan iÃ§erikler Microsoft Word (.docx) formatÄ±nda kaydedilir
    - Her sayfa iÃ§in ayrÄ± bir dosya oluÅŸturulur
    - Dosya isimleri URL yapÄ±sÄ±na gÃ¶re otomatik oluÅŸturulur
    
    ### âš ï¸ Ã–nemli Notlar
    - BazÄ± siteler otomatik kazÄ±mayÄ± engelleyebilir (CAPTCHA, IP engelleme vb.)
    - Site kazÄ±ma yasal sÄ±nÄ±rlamalara dikkat edilerek yapÄ±lmalÄ±dÄ±r
    - Ã‡ok fazla sayfa kazÄ±manÄ±z durumunda iÅŸlem uzun sÃ¼rebilir
    - Benzer iÃ§erikler otomatik olarak atlanÄ±r
    """)
    
    with st.expander("ğŸ’¡ Ä°puÃ§larÄ± ve PÃ¼f Noktalar"):
        st.markdown("""
        - **Site AÄŸacÄ±nÄ± Anlamak**: Ã–nce tek URL modunda birkaÃ§ sayfa deneyerek iÃ§erik yapÄ±sÄ±nÄ± analiz edin
        - **DoÄŸru Element SeÃ§imi**: 
          - Gereksiz elementleri iÅŸaretlemeyin, bu iÅŸlemi hÄ±zlandÄ±rÄ±r
          - **Span Ä°Ã§erikleri** seÃ§eneÄŸini sadece accordion/soru-cevap bÃ¶lÃ¼mleri iÃ§in aÃ§Ä±n
        - **Derinlik AyarÄ±**: BÃ¼yÃ¼k sitelerde derinliÄŸi dÃ¼ÅŸÃ¼k tutun (1-2), aksi halde iÅŸlem Ã§ok uzayabilir
        - **URL YapÄ±sÄ±**: BazÄ± sitelerde parametre iÃ§eren URL'ler (? iÅŸareti ile baÅŸlayan) aynÄ± iÃ§eriÄŸi farklÄ± URL'lerle sunar
        - **Filtreleme**: SonuÃ§ klasÃ¶rÃ¼nde elde edilen dosyalarÄ± iÃ§erik aÃ§Ä±sÄ±ndan kontrol edin
        """)
    
    with st.expander("ğŸ› ï¸ Sorun Giderme"):
        st.markdown("""
        - **BaÄŸlantÄ± HatalarÄ±**: Site eriÅŸilebilir olduÄŸundan emin olun
        - **Ä°Ã§erik BulunamadÄ±**: 
          - SeÃ§tiÄŸiniz element tÃ¼rlerini geniÅŸletin
          - **Span iÃ§eren accordionlar** iÃ§in "Span Ä°Ã§erikleri" seÃ§eneÄŸini aÃ§Ä±n
        - **YavaÅŸ Ã‡alÄ±ÅŸma**: DerinliÄŸi ve maksimum sayfa sayÄ±sÄ±nÄ± azaltÄ±n
        - **BoÅŸ Dosyalar**: 
          - Sitenin iÃ§erik yapÄ±sÄ± farklÄ± olabilir, elementleri tekrar kontrol edin
          - Ã–zel iÃ§erikler (accordionlar) iÃ§in doÄŸru element seÃ§imini yaptÄ±ÄŸÄ±nÄ±zdan emin olun
        - **Eksik Soru-Cevap Ä°Ã§eriÄŸi**:
          - "Span Ä°Ã§erikleri" seÃ§eneÄŸini aÃ§Ä±n
          - "Div Ä°Ã§erikleri" seÃ§eneÄŸiyle birlikte deneyin
          - Filtreleme seÃ§eneklerini geÃ§ici olarak kapatÄ±p test edin
        """)
    
    with st.expander("ğŸ” Ã–zel Ä°Ã§erikler (Accordion/SSS) Yakalama"):
        st.markdown("""
        ### Span Ä°Ã§indeki Accordionlar Ä°Ã§in:
        1. **Span Ä°Ã§erikleri** seÃ§eneÄŸini aÃ§Ä±n
        2. **Gereksiz Divleri Filtrele** seÃ§eneÄŸi aÃ§Ä±k kalabilir
        3. AÅŸaÄŸÄ±daki yapÄ±larÄ± otomatik tanÄ±r:
           - Class veya ID'sinde ÅŸu kelimeler geÃ§enler:
             - `accordion`, `faq`, `soru`, `cevap`, `sss`
             - `question`, `answer`, `collapsible`, `toggle`
             - `sÄ±kÃ§a-sorulan`, `vs-soru`, `faq-item`
        
        ### Div Ä°Ã§indeki Accordionlar Ä°Ã§in:
        1. **Div Ä°Ã§erikleri** seÃ§eneÄŸini aÃ§Ä±n
        2. YukarÄ±daki anahtar kelimeleri iÃ§eren divler otomatik alÄ±nÄ±r
        
        ### Tavsiyeler:
        - Ã–nce tek sayfada test yapÄ±n
        - FarklÄ± element kombinasyonlarÄ±nÄ± deneyin
        - Ã‡ok fazla span iÃ§eriÄŸi alÄ±nÄ±yorsa, "Span Ä°Ã§erikleri"ni kapatÄ±p sadece divleri deneyin
        """)

# Footer
st.markdown("---")
st.markdown("### ğŸ› ï¸ Web KazÄ±ma AracÄ± vSpecial :) | GeliÅŸtirici: Emir TÃ¼rker")
st.caption("Bu aracÄ± QmindLab'e Ã¶zeldir.")